#+title: Queueing Theory and Simulation, lecture 4
#+author: Nicky van Foreest
#+date: \today
#+email: n.d.van.foreest@rug.nl


#+STARTUP: overview
#+OPTIONS:  toc:1 H:2

#+PROPERTY: header-args    :session

# +include: preamble_presentation.org
#+include: preamble_handout.org



* General things
** General things
- Assignments: please sign up in time, we have some 100 groups\ldots
- Assignments: if too late, max grade a 6.
- Tutorials


* Concepts: Rate, stability, load, section 3.3
** Why long-run characterization?
- We have recursions to characterization of the state of a queueing at a particular time
- To know the state at any time, compute the full recursion up to that point. This is a lot of work
- There is no simple expression for the state as a function of time
- we focus on time-averages

** Arrival Rate
See the book for the details. This is a sketch of ideas.
\begin{align}
\{X_{k}\} & \quad\text{Basis data}\\
A_{k} &= A_{k-1} + X_{k},\\
A(t) &= \max\{n : A_{n} \leq t\} \\
\lambda &= \lim_{t\to\infty} \frac{A(t)}{t} \\
\frac{A_{n}}{n} &=\frac{A_n}{A(A_{n})} \approx \frac{t}{A(t)} \\
\E X &= \lim_{n\to\infty} \frac{ 1}{n}\sum_{k=1}^{n} X_{k} = \lim_{n\to\infty} \frac{A_{n}}{n} = \frac{1}{\lambda}.
\end{align}

** Departure rate
\begin{align}
\label{eq:1}
D(t) &\leq A(t) \\
&\implies \\
\delta &= \lim_{t\to\infty}\frac{D(t)}{t} \leq \lim_{t\to\infty}\frac{A(t)}{t}  = \lambda.
\end{align}

** Service rate
\begin{align}
\{S_{k}\} &\quad \text{Basis data}\\
U_{k} &= U_{k-1} + U_{k},\\
U(t) &= \max\{n : U_{n} \leq t\} \\
\mu &= \lim_{t\to\infty} \frac{U(t)}{t} \\
\E S &= \lim_{n\to\infty} \frac{ 1}{n}\sum_{k=1}^{n} U_{k} =  \frac{1}{\mu}.
\end{align}

** Load and utilization
- load: $\lambda \E S$, rate at which work arrives
- utilization: $\rho = \lambda \E S/ c$ for the $G/G/c$ queue
- only when $c=1$: $\rho$ is equal to the load.

* KPIs, section 3.4

** KPIs  sampled at/observed by job arrival times.

\begin{align}
\label{eq:2}
\E W &= \lim_{n\to\infty} \frac{1}{n} \sum_{k=1}^n W_{k} \\
\P{W\leq x} &= \lim_{n\to\infty} \frac{1}{n} \sum_{k=1}^n \1{W_{k}\leq x} \\
\end{align}

** KPIs time averages
\begin{align}
\label{eq:3}
L(s) &= A(s) - D(s) \\
\E L &= \lim_{t\to\infty} \frac{1}{t}\int_{0}^{t} L(s) \d s,
\end{align}
We can also define the average as seen by job arrivals:
\begin{align}
\label{eq:4}
L(k) &= A(A_{k}) - D(A_{k}) \\
\E L &= \lim_{n\to\infty} \frac{1}{n} \sum_{k=1}^n L_{k} \\
\end{align}
These two $\E L$ are not necessarily the same. As yet I haven't found good notation to make a clear distinction. Be /very/  careful!

* Convergence to steady state
** Dealing with random variables
#+begin_src python
W = RV({5: 1})
X = RV({1: 1 / 3, 2: 1 / 3, 4: 1 / 3,})
S = RV({1: 1 / 3, 2: 1 / 3, 3: 1 / 3,})

for n in range(1, 21):
    W += S - X
    W = W.plus()
#+end_src
Recall:
\begin{align*}
\P{S-X = i} = \sum_{k} \P{S=i+k}\P{X=k}.
\end{align*}
Thus,  I need a $+$ operator for $X+Y$, a $-$ operator for $X-Y$, and a function $[X]^{+} = \max\{X, 0\}$.

** Python inheritance to the rescue
#+begin_src python
from collections import defaultdict
import operator


class RV(defaultdict):
    def __init__(self, p=None):
        super().__init__(float)
        if p:
            for (i, pi,) in p.items():
                self[i] = pi
#+end_src

** Addition
#+begin_src python
    def apply_operator(self, Y, op):
        R = RV()
        for (i, pi,) in self.items():
            for (j, pj,) in Y.items():
                R[op(i, j)] += pi * pj
        return R

    def __add__(self, X):
        return self.apply_operator(X, operator.add)
#+end_src


** Plus

#+begin_src python
    def apply_function(self, h):
        R = RV()
        for (i, pi,) in self.items():
            R[h(i)] += pi
        return R


    def plus(self):
        return self.apply_function(lambda x: max(x, 0))

#+end_src

** Confession
- For fully working code, see the book.
- With some python knowledge it's fairly easy to compute $\{W_{k}\}$.  \pause
- Now you have to confess that all this abstraction in python is fantastic. So, please type in the chat box how much you like this :-)
